{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91d2f2c5-097f-4f02-b9dd-9365d6a23db6",
   "metadata": {},
   "source": [
    "# Build Embeddings\n",
    "\n",
    "## Dataset\n",
    "\n",
    "First we need to download the YT transcriptions dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e58ca384-1f49-4565-ae1b-67c046dc38a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.cache/pypoetry/virtualenvs/chatgpt-retrieval-plugin-PzAOZGE--py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset json (/home/codespace/.cache/huggingface/datasets/jamescalam___json/jamescalam--youtube-transcriptions-af3de6eb3e148fe9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'visibility', 'published', 'url', 'id', 'text', 'start', 'end'],\n",
       "    num_rows: 27214\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset  # !pip install datasets\n",
    "\n",
    "yt_transcriptions_ds = load_dataset(\n",
    "    \"jamescalam/youtube-transcriptions\",\n",
    "    split=\"train\",\n",
    "  \trevision=\"8dca835\"\n",
    ")\n",
    "yt_transcriptions_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a4f743c-4eb2-4e8d-b8a4-86f074f5d8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n",
       " 'visibility': 'Public',\n",
       " 'published': '2021-07-06 13:00:03 UTC',\n",
       " 'url': 'https://youtu.be/35Pdoyi6ZoQ',\n",
       " 'id': '35Pdoyi6ZoQ-t0.0',\n",
       " 'text': 'Hi, welcome to the video.',\n",
       " 'start': 0.0,\n",
       " 'end': 9.36}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_transcriptions_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a70451b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n",
       " 'visibility': 'Public',\n",
       " 'published': '2021-07-06 13:00:03 UTC',\n",
       " 'url': 'https://youtu.be/35Pdoyi6ZoQ',\n",
       " 'id': '35Pdoyi6ZoQ-t3.0',\n",
       " 'text': 'So this is the fourth video in a Transformers',\n",
       " 'start': 3.0,\n",
       " 'end': 11.56}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_transcriptions_ds[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56ec200e-3a1d-4dce-b3b7-08f72bb13c02",
   "metadata": {},
   "source": [
    "The sentences are all quite short at the moment, we need to merge them to create better chunks of text containing more meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f2876d-9e4a-4271-aeb7-4ac8afad7c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9072/9072 [00:08<00:00, 1083.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "new_data = []\n",
    "\n",
    "window = 6  # number of sentences to combine\n",
    "stride = 3  # number of sentences to 'stride' over, used to create overlap\n",
    "url_counter_part = 0\n",
    "\n",
    "for i in tqdm(range(0, len(yt_transcriptions_ds), stride)):\n",
    "    i_end = min(len(yt_transcriptions_ds)-1, i+window)\n",
    "\n",
    "    if yt_transcriptions_ds[i]['title'] != yt_transcriptions_ds[i_end]['title']:\n",
    "        url_counter_part = 0\n",
    "        # in this case we skip this entry as we have start/end of two videos\n",
    "        continue\n",
    "    \n",
    "    url_id = yt_transcriptions_ds[i]['url'].split('/')[-1]\n",
    "    segment_id = f\"{url_id}.{url_counter_part}\"\n",
    "    url_counter_part += 1\n",
    "\n",
    "    text = ' '.join(yt_transcriptions_ds[i:i_end]['text'])\n",
    "    new_data.append({\n",
    "        'start': yt_transcriptions_ds[i]['start'],\n",
    "        'end': yt_transcriptions_ds[i_end]['end'],\n",
    "        'title': yt_transcriptions_ds[i]['title'],\n",
    "        'text': text,\n",
    "        'id': segment_id,\n",
    "        'url': yt_transcriptions_ds[i]['url'],\n",
    "        'published': yt_transcriptions_ds[i]['published']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88c798ca-6aef-41a2-9979-056561ca693e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 0.0,\n",
       " 'end': 25.76,\n",
       " 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n",
       " 'text': \"Hi, welcome to the video. So this is the fourth video in a Transformers from Scratch mini series. So if you haven't been following along, we've essentially covered what you can see on the screen. So we got some data.\",\n",
       " 'id': '35Pdoyi6ZoQ.0',\n",
       " 'url': 'https://youtu.be/35Pdoyi6ZoQ',\n",
       " 'published': '2021-07-06 13:00:03 UTC'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d09cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 11.56,\n",
       " 'end': 35.96,\n",
       " 'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n",
       " 'text': \"So if you haven't been following along, we've essentially covered what you can see on the screen. So we got some data. We built a tokenizer with it. And then we've set up our input pipeline ready to begin actually training our model, which\",\n",
       " 'id': '35Pdoyi6ZoQ.1',\n",
       " 'url': 'https://youtu.be/35Pdoyi6ZoQ',\n",
       " 'published': '2021-07-06 13:00:03 UTC'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b80057f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 1090.0,\n",
       " 'end': 1106.0,\n",
       " 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)',\n",
       " 'text': \"So let's expand that out a little bit. So we'll go with token type IDs. Let's go with number 0.\",\n",
       " 'id': 'x1lAcT3xl5M.176',\n",
       " 'url': 'https://youtu.be/x1lAcT3xl5M',\n",
       " 'published': '2021-05-27 16:15:39 UTC'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[499]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a49ea79-b7f7-4f74-9b79-09152fc21b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 1096.0,\n",
       " 'end': 1112.0,\n",
       " 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)',\n",
       " 'text': \"token type IDs. Let's go with number 0. Okay. So now we see okay the reason is because they're in the middle here.\",\n",
       " 'id': 'x1lAcT3xl5M.177',\n",
       " 'url': 'https://youtu.be/x1lAcT3xl5M',\n",
       " 'published': '2021-05-27 16:15:39 UTC'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[500]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a0496cd-ea41-4db4-b41a-98a495acaf05",
   "metadata": {},
   "source": [
    "## Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b1d1ae0-6f9d-4906-b84c-7bee0985b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "import os\n",
    "\n",
    "index_id = \"youtube-search\"\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
    "    environment=os.getenv(\"PINECONE_ENV\")\n",
    ")\n",
    "\n",
    "# if index_id not in pinecone.list_indexes():\n",
    "#     pinecone.create_index(\n",
    "#         index_id,\n",
    "#         dim,\n",
    "#         metric=\"dotproduct\"\n",
    "#     )\n",
    "\n",
    "index = pinecone.Index(index_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "902637cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'namespaces': {}, 'dimension': 1536, 'index_fullness': 0.0, 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "stats = index.describe_index_stats().to_dict()\n",
    "print(stats)\n",
    "\n",
    "if stats[\"total_vector_count\"] > 0:\n",
    "    index.delete(deleteAll='true')\n",
    "    stats = index.describe_index_stats().to_dict()\n",
    "    print(stats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "828756a7-08a2-45be-a329-46100aced366",
   "metadata": {},
   "source": [
    "Now let's begin building the embeddings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "662513ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0.0,\n",
       "  'end': 25.76,\n",
       "  'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n",
       "  'text': \"Hi, welcome to the video. So this is the fourth video in a Transformers from Scratch mini series. So if you haven't been following along, we've essentially covered what you can see on the screen. So we got some data.\",\n",
       "  'id': '35Pdoyi6ZoQ.0',\n",
       "  'url': 'https://youtu.be/35Pdoyi6ZoQ',\n",
       "  'published': '2021-07-06 13:00:03 UTC'},\n",
       " {'start': 11.56,\n",
       "  'end': 35.96,\n",
       "  'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n",
       "  'text': \"So if you haven't been following along, we've essentially covered what you can see on the screen. So we got some data. We built a tokenizer with it. And then we've set up our input pipeline ready to begin actually training our model, which\",\n",
       "  'id': '35Pdoyi6ZoQ.1',\n",
       "  'url': 'https://youtu.be/35Pdoyi6ZoQ',\n",
       "  'published': '2021-07-06 13:00:03 UTC'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cd0413e-ee5a-41b8-a5e1-1298cce62d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [01:34<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import openai\n",
    "\n",
    "# we encode and insert in batches of 64\n",
    "batch_size = 64\n",
    "\n",
    "# loop through in batches of 64\n",
    "for i in tqdm(range(0, len(new_data), batch_size)):\n",
    "    # find end position of batch (for when we hit end of data)\n",
    "    i_end = min(len(new_data)-1, i+batch_size)\n",
    "    # extract the metadata like text, start/end positions, etc\n",
    "    batch_meta = [{\n",
    "        \"text\": new_data[x][\"text\"],\n",
    "        \"start\": new_data[x][\"start\"],\n",
    "        \"end\": new_data[x][\"end\"],\n",
    "        \"url\": new_data[x][\"url\"],\n",
    "        \"title\": new_data[x][\"title\"]\n",
    "    } for x in range(i, i_end)]\n",
    "    # extract only text to be encoded by embedding model\n",
    "    batch_text = [row['text'] for row in new_data[i:i_end]]\n",
    "    # create the embedding vectors\n",
    "    res = openai.Embedding.create(input=batch_text, engine=\"text-embedding-ada-002\")\n",
    "    batch_embeds = [record['embedding'] for record in res['data']]\n",
    "    # extract IDs to be attached to each embedding and metadata\n",
    "    batch_ids = [row['id'] for row in new_data[i:i_end]]\n",
    "    # 'upsert' (eg insert) IDs, embeddings, and metadata to index\n",
    "    to_upsert = list(zip(batch_ids, batch_embeds, batch_meta))\n",
    "    index.upsert(to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a32f2d85-a310-4b97-bcf2-5993efecb784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 8857}},\n",
       " 'total_vector_count': 8857}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e1c5458-c25a-4150-adc2-e783bcbe2b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'matches': [{'id': 'fGwH2YoQkDM.0',\n",
       "                           'metadata': {'end': 77.2,\n",
       "                                        'start': 17.36,\n",
       "                                        'text': 'is Clip which was built and '\n",
       "                                                'trained by OpenAI. Now Clip '\n",
       "                                                'is open source and it has '\n",
       "                                                'been around for a little '\n",
       "                                                \"while. It's been around from \"\n",
       "                                                'since the start of 2021 but '\n",
       "                                                \"in the past few months we've \"\n",
       "                                                'seen the adoption of Clip '\n",
       "                                                'grow at a pretty insane rate. '\n",
       "                                                'It has found uses in a load '\n",
       "                                                'of things so what we will be '\n",
       "                                                'focusing on is text or '\n",
       "                                                'multimodal search across text '\n",
       "                                                \"and images. We're also going \"\n",
       "                                                'to talk a little bit about '\n",
       "                                                'zero shot classifications, '\n",
       "                                                'zero shot object detection '\n",
       "                                                'and at some point in the '\n",
       "                                                \"future we'll also talk about \"\n",
       "                                                'the use of models like Clip '\n",
       "                                                'or even Clip',\n",
       "                                        'title': 'CLIP Explained | Multi-modal '\n",
       "                                                 'ML',\n",
       "                                        'url': 'https://youtu.be/fGwH2YoQkDM'},\n",
       "                           'score': 0.888281465,\n",
       "                           'values': []},\n",
       "                          {'id': 'coaaSxys5so.7',\n",
       "                           'metadata': {'end': 176.0,\n",
       "                                        'start': 131.0,\n",
       "                                        'text': \"So this one's not even really \"\n",
       "                                                \"a question. I'm just going to \"\n",
       "                                                'say OpenAI Clip. And what I '\n",
       "                                                'want to do is just say okay '\n",
       "                                                'can you summarize what OpenAI '\n",
       "                                                \"Clip is. So we'll come down \"\n",
       "                                                \"here. Let's see what it \"\n",
       "                                                'returns. Cool so OpenAI Clip '\n",
       "                                                'is a contrastive language '\n",
       "                                                'image pre-training model that '\n",
       "                                                'uses pairs of images and text '\n",
       "                                                'and returns a matrix of '\n",
       "                                                'cosine similarity between '\n",
       "                                                'text and each image.',\n",
       "                                        'title': 'How to build next-level Q&A '\n",
       "                                                 'with OpenAI',\n",
       "                                        'url': 'https://youtu.be/coaaSxys5so'},\n",
       "                           'score': 0.884071946,\n",
       "                           'values': []},\n",
       "                          {'id': '98POYg2HZqQ.19',\n",
       "                           'metadata': {'end': 424.64,\n",
       "                                        'start': 374.96000000000004,\n",
       "                                        'text': \"this sort of thing is. Let's \"\n",
       "                                                'talk about how CLIP makes '\n",
       "                                                'zero-shot learning so '\n",
       "                                                'effective. So CLIP stands for '\n",
       "                                                'Contrastive Language Image '\n",
       "                                                'Pre-training. It was released '\n",
       "                                                'by OpenAI in 2021 and since '\n",
       "                                                'then it has done pretty well. '\n",
       "                                                'We can find it in a lot of '\n",
       "                                                'different use cases this is '\n",
       "                                                'just one of them. So CLIP '\n",
       "                                                'itself actually consists of '\n",
       "                                                \"two models. I've discussed \"\n",
       "                                                'this in a previous video and '\n",
       "                                                'article in a lot more detail '\n",
       "                                                \"so if you're interested go \"\n",
       "                                                'and have a look at that. For '\n",
       "                                                \"now we're going to keep \"\n",
       "                                                'things pretty light on how '\n",
       "                                                'CLIP works but in this '\n",
       "                                                'version of CLIP',\n",
       "                                        'title': 'Zero-Shot Image '\n",
       "                                                 \"Classification with OpenAI's \"\n",
       "                                                 'CLIP',\n",
       "                                        'url': 'https://youtu.be/98POYg2HZqQ'},\n",
       "                           'score': 0.881624043,\n",
       "                           'values': []},\n",
       "                          {'id': '98POYg2HZqQ.18',\n",
       "                           'metadata': {'end': 405.84,\n",
       "                                        'start': 354.96,\n",
       "                                        'text': 'particular data sets or '\n",
       "                                                'benchmarks. Other than one '\n",
       "                                                'surprisingly without seeing '\n",
       "                                                'any training data for this '\n",
       "                                                'particular data set, CLIP did '\n",
       "                                                'actually get state-of-the-art '\n",
       "                                                'performance on that one data '\n",
       "                                                'set which is surprising '\n",
       "                                                'without seeing any of the '\n",
       "                                                'training data but here we go '\n",
       "                                                'this is how useful this sort '\n",
       "                                                \"of thing is. Let's talk about \"\n",
       "                                                'how CLIP makes zero-shot '\n",
       "                                                'learning so effective. So '\n",
       "                                                'CLIP stands for Contrastive '\n",
       "                                                'Language Image Pre-training. '\n",
       "                                                'It was released by OpenAI in '\n",
       "                                                '2021 and since then it has '\n",
       "                                                'done pretty well. We can find '\n",
       "                                                'it in a lot of different use '\n",
       "                                                'cases this is just one of '\n",
       "                                                'them.',\n",
       "                                        'title': 'Zero-Shot Image '\n",
       "                                                 \"Classification with OpenAI's \"\n",
       "                                                 'CLIP',\n",
       "                                        'url': 'https://youtu.be/98POYg2HZqQ'},\n",
       "                           'score': 0.86889559,\n",
       "                           'values': []},\n",
       "                          {'id': '989aKUVBfbk.3',\n",
       "                           'metadata': {'end': 119.0,\n",
       "                                        'start': 88.5,\n",
       "                                        'text': 'Anything to do with images '\n",
       "                                                \"and text there's a good \"\n",
       "                                                'chance we can do it with '\n",
       "                                                \"clip. So let's have a look at \"\n",
       "                                                'how we actually use clip. '\n",
       "                                                'OpenAI released a GitHub '\n",
       "                                                'repository OpenAI clip here. '\n",
       "                                                \"This contains clip but we're \"\n",
       "                                                'not going to use this '\n",
       "                                                \"implementation. We're \"\n",
       "                                                'actually going to use this '\n",
       "                                                'implementation of clip. So '\n",
       "                                                'this is on Hugging Face.',\n",
       "                                        'title': 'Fast intro to multi-modal ML '\n",
       "                                                 \"with OpenAI's CLIP\",\n",
       "                                        'url': 'https://youtu.be/989aKUVBfbk'},\n",
       "                           'score': 0.862437844,\n",
       "                           'values': []}],\n",
       "              'namespace': ''}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is OpenAI's CLIP?\"\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "query_vectors = [embeddings.embed_query(query)]\n",
    "\n",
    "# xq = openai.Embedding.create(input=query, engine=MODEL)['data'][0]['embedding']\n",
    "# index.query(vector=xq, top_k=5, include_metadata=True)\n",
    "\n",
    "index.query(queries=query_vectors, top_k=5, include_metadata=True)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
